# lakekeeper.yaml — Lakekeeper configuration
# Generated by: lakekeeper generate-config
# Copy this file, edit the values, then run:
#   lakekeeper --config-file lakekeeper.yaml compact --database mydb
# Full documentation: https://github.com/ab2dridi/Lakekeeper

# ─────────────────────────────────────────────────────────────────────────────
# Core compaction parameters
# ─────────────────────────────────────────────────────────────────────────────

# Target HDFS block size in MB.
# Lakekeeper computes: target_files = ceil(table_size_bytes / block_size_bytes).
# Match the value configured in your cluster (check hdfs-site.xml:
# dfs.blocksize; common values are 128 or 256).
block_size_mb: 128

# Compaction threshold ratio.
# A partition is compacted when:
#   average_file_size < block_size_mb / compaction_ratio_threshold
# Default (10.0): compact when avg file < 12.8 MB for 128 MB blocks.
# Lower value → more aggressive compaction (fewer, larger files tolerated).
# Higher value → less aggressive (only very small files trigger compaction).
compaction_ratio_threshold: 10.0

# Prefix used for backup table names in the Hive Metastore.
# A backup of mydb.events becomes: mydb.__bkp_events_YYYYMMDD_HHMMSS
# Change this only if "__bkp" conflicts with an existing naming convention.
backup_prefix: __bkp

# Logging verbosity. One of: DEBUG | INFO | WARNING | ERROR
log_level: INFO

# Run ANALYZE TABLE COMPUTE STATISTICS after each successful compaction.
# Updates row count, file count, and total size in the Hive Metastore so
# that the query planner uses accurate statistics for the freshly compacted
# data. For partitioned tables, statistics are refreshed per compacted
# partition and then at the table level.
# This adds one Metastore round-trip per compacted partition plus one
# table-level call — negligible cost compared to the compaction itself.
analyze_after_compaction: false

# ─────────────────────────────────────────────────────────────────────────────
# Sort order preservation (optional)
# ─────────────────────────────────────────────────────────────────────────────
# coalesce() does NOT preserve sort order. If your pipelines write files
# pre-sorted on specific columns (e.g. for predicate-pruning efficiency),
# uncomment this section to re-apply the sort before coalescing.
#
# Priority for each table (highest → lowest):
#   1. CLI --sort-columns col1,col2   (overrides everything)
#   2. YAML sort_columns below        (per-table, applied to all runs)
#   3. DDL SORTED BY auto-detection   (zero-config if table has CLUSTERED/SORTED)
#   4. No sort (plain coalesce)
#
# WARNING: sorting triggers a full Spark shuffle before coalescing.
# This increases execution time and executor memory usage.
# Only enable it for tables where sort order materially improves
# downstream query performance (e.g. time-series, heavily filtered datasets).
#
# sort_columns:
#   mydb.events: [date, user_id]   # sort events by date then user_id
#   mydb.logs:   [year, month, day]

# ─────────────────────────────────────────────────────────────────────────────
# Automatic spark-submit (required on Kerberized YARN clusters)
# ─────────────────────────────────────────────────────────────────────────────
# When enabled: true, "lakekeeper compact ..." automatically builds and
# executes the spark-submit command below, then exits with the same return
# code. A sentinel env var (LAKEKEEPER_SUBMITTED=1) prevents re-submission
# when the YARN driver calls the script a second time.
#
# When enabled: false (default), lakekeeper creates a local SparkSession —
# suitable for development or clusters where lakekeeper runs inside a
# spark-submit script directly.
spark_submit:
  enabled: false

  # Binary name for the spark-submit executable.
  # On Cloudera CDP clusters where "spark-submit" points to Spark 2,
  # use "spark3-submit" to explicitly target Spark 3.
  submit_command: spark-submit

  # Spark master URL. Use "yarn" for YARN clusters.
  master: yarn

  # Deploy mode:
  #   client  — driver runs on the edge node. Simpler; can read local files
  #             (config, keytab). Recommended for interactive use.
  #   cluster — driver runs inside a YARN container on the cluster. Better
  #             for long-running jobs; config file is shipped automatically.
  deploy_mode: client

  # Kerberos authentication — required on Kerberized clusters.
  # principal: myuser@MY.REALM.COM
  # keytab: /etc/security/keytabs/myuser.keytab

  # YARN queue to submit the job to.
  # queue: data-engineering

  # Conda-packed Python environment to ship to the cluster.
  # Build with:  conda create -n lakekeeper_env python=3.9 -y
  #              conda activate lakekeeper_env && pip install lakekeeper
  #              conda-pack -o lakekeeper_env.tar.gz
  # Upload with: hdfs dfs -put lakekeeper_env.tar.gz /opt/
  # archives: /opt/lakekeeper_env.tar.gz#lakekeeper_env

  # Python interpreter inside the unpacked archive (set via spark.pyspark.python).
  # python_env: ./lakekeeper_env/bin/python

  # Executor and driver resources.
  # executor_memory: 4g
  # num_executors: 10
  # executor_cores: 2
  # driver_memory: 2g

  # Path to the entry-point script on HDFS or local filesystem.
  # Upload with: hdfs dfs -put run_lakekeeper.py /opt/lakekeeper/
  script_path: run_lakekeeper.py

  # Additional Spark configuration key-value pairs (passed as --conf key=value).
  # extra_conf:
  #   spark.yarn.kerberos.relogin.period: 1h
  #   spark.yarn.security.tokens.hive.enabled: "false"
  #   spark.sql.shuffle.partitions: "400"

  # Extra files to distribute to the driver container via --files.
  # Useful in deploy_mode: cluster to ship Hive/HDFS config files that the
  # remote driver cannot read from the edge node's filesystem.
  # extra_files:
  #   - /etc/hive/conf.cloudera.hive/hive-site.xml
  #   - /etc/hive/conf.cloudera.hive/hdfs-site.xml

  # Python packages or wheels distributed to executors via --py-files.
  # py_files:
  #   - /opt/mypackage.whl
